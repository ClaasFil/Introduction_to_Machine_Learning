{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de1a216c",
   "metadata": {},
   "source": [
    "# Task 1: Movie Recommendation\n",
    "## Subtask 1: Data Loading and Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33224bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a152d5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading csv files\n",
    "def loadData():\n",
    "    colnamesMovies=['MovieID', 'Titel', 'Genres'] \n",
    "    movies =  pd.read_csv('movies.dat', sep=\"::\", skiprows=0, engine='python', names=colnamesMovies, encoding='latin').to_numpy()\n",
    "\n",
    "    colnamesRating=['UserID', 'MovieID', 'Rating', 'Timestamp'] \n",
    "    ratings =  pd.read_csv('ratings.dat', sep=\"::\", skiprows=0, engine='python', names=colnamesRating)\n",
    "\n",
    "    colnamesUsers=['UserID', 'Gender', 'Age', 'Occupation', 'ZIP'] \n",
    "    users =  pd.read_csv('users.dat', sep=\"::\", skiprows=0, engine='python', names=colnamesUsers).to_numpy()\n",
    "    return movies, ratings, users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a28b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "genrePositions = dict([\n",
    "    ('Action', '0'),\n",
    "    ('Adventure', '1'),\n",
    "    ('Animation', '2'),\n",
    "    ('Children\\'s', '3'),\n",
    "    ('Comedy', '4'),\n",
    "    ('Crime', '5'),\n",
    "    ('Documentary', '6'),\n",
    "    ('Drama', '7'),\n",
    "    ('Fantasy', '8'),\n",
    "    ('Film-Noir', '9'),\n",
    "    ('Horror', '10'),\n",
    "    ('Musical', '11'),\n",
    "    ('Mystery', '12'),\n",
    "    ('Romance', '13'),\n",
    "    ('Sci-Fi', '14'),\n",
    "    ('Thriller', '15'),\n",
    "    ('War', '16'),\n",
    "    ('Western', '17')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cde71c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def constructData(movies, ratings, users):\n",
    "    \n",
    "    #decode genres in to individual Pandadf columns  \n",
    "    genres = np.zeros([movies.shape[0], 18])\n",
    "    #loop over every movie\n",
    "    for i, eachMovie in enumerate(movies):   \n",
    "        #split the single  genres of a movie\n",
    "        genresPerson = (eachMovie[2].split(\"|\"))\n",
    "        #set the corresponding column to 1 if the movie belongs to the genre\n",
    "        for eachGenre in genresPerson:\n",
    "            genres[i,int(genrePositions.get(eachGenre))] = 1\n",
    "    \n",
    "    #cast to pandas\n",
    "    genresDf = pd.DataFrame(genres, columns = ['Action','Adventure','Animation','Children\\'s','Comedy','Crime','Documentary','Drama','Fantasy','Film-Noir','Horror','Musical','Mystery','Romance','Sci-Fi','Thriller','War','Western'])\n",
    "    genresDf['MovieID'] =  movies[:,0]  \n",
    "    \n",
    "    #decode occupations in to individual Pandadf colloumns \n",
    "    ocupations = np.zeros([users.shape[0], 21])\n",
    "    for i, user in enumerate(users):\n",
    "        ocupations[i,int(user[3])-1] = 1\n",
    "    #cast to pandas\n",
    "    ocupationsDf = pd.DataFrame(ocupations, columns = ['other', 'academic/educator', 'artist', 'clerical/admin', 'college/grad student', 'customer service', 'doctor/health care', 'executive/managerial', 'farmer', 'homemaker',\"K-12 student\", 'lawyer', 'programmer', 'retired','sales/marketing','scientist','self-employed','technician/engineer','tradesman/craftsman', 'unemployed', 'writer'])\n",
    "    ocupationsDf['UserID'] =  users[:,0]  \n",
    "    \n",
    "    #casting movies and users into dict for better data combination\n",
    "    moviesTitel = {movie[0]:movie[1] for movie in movies}\n",
    "    usersGender = {user[0]:user[1] for user in users}\n",
    "    usersAge = {user[0]:user[2] for user in users}\n",
    "    \n",
    "    #Creating final dataframe \n",
    "    # dicts with the mapping command and merge df into each other\n",
    "    ratings['Movie Titel'] = ratings['MovieID'].map(moviesTitel)\n",
    "    ratings = pd.merge(ratings,genresDf, on='MovieID')\n",
    "    ratings['Gender'] = ratings['UserID'].map(usersGender)\n",
    "    ratings['Age'] = ratings['UserID'].map(usersAge)\n",
    "    ratings = pd.merge(ratings,ocupationsDf, on='UserID')\n",
    "    \n",
    "    #enable better model training by working with integer\n",
    "    ratings.loc[ratings[\"Gender\"] == \"M\", \"Gender\"] = 2\n",
    "    ratings.loc[ratings[\"Gender\"] == \"F\", \"Gender\"] = 1\n",
    "    \n",
    "    \n",
    "    #remove users with less then 100 rtings \n",
    "    byUserID = ratings.groupby('UserID').aggregate(np.count_nonzero)\n",
    "    tags = byUserID[byUserID.MovieID >= 100].index\n",
    "    ratings = (ratings[ratings['UserID'].isin(tags)])\n",
    "    \n",
    "    #sorted by user ID to enable easy train test split\n",
    "    ratings = ratings.sort_values('UserID')        \n",
    "\n",
    "    #delete not needed features\n",
    "    del ratings['MovieID']\n",
    "    del ratings['Timestamp']\n",
    "    del ratings['Movie Titel']\n",
    "\n",
    "    #Train test split\n",
    "    testData = ratings.loc[ratings['UserID'] <= 1000]\n",
    "    del testData['UserID']\n",
    "    trainData = ratings.loc[ratings['UserID'] >= 1000]\n",
    "    del trainData['UserID']\n",
    "\n",
    "    \n",
    "    #separating the labels from the corresponding features\n",
    "    trainData = trainData.to_numpy().astype('float64')  \n",
    "    testData = testData.to_numpy().astype('float64')  \n",
    "    X_test = testData[:,1:]\n",
    "    y_test = testData[:,0]\n",
    "    X_train = trainData[:,1:]\n",
    "    y_train = trainData[:,0]\n",
    "    return X_test, y_test, X_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3effc4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies, ratings, users = loadData() #loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66876f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, X_train, y_train = constructData(movies, ratings, users) # constructing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "764ba01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128025, 41)\n",
      "(128025,)\n",
      "(719277, 41)\n",
      "(719277,)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22353a87",
   "metadata": {},
   "source": [
    "In the report, describe the feature vectors you created. Explain how you represented features and why? <br>\n",
    "Age, Gender, Occupation, and Genre are kept as features. Occupation and Genres however are split up into a feature for every possible vlaule. This separation is done to enable easier training to put additional weight on these important features. <br>\n",
    "How many samples do your training and test data contain? <br>\n",
    "The Dataset has 128025 Test samples and 719277 training samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a3c09b",
   "metadata": {},
   "source": [
    "## Subtask 2: Basic Movie Recommendation\n",
    "### linear SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5dfe0d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:284: ConvergenceWarning: Solver terminated early (max_iter=20).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter of SVC:{'C': 0.1}\n",
      "Tain accuracy: 0.2497076925857493\n",
      "Test accuarcy: 0.2538488576449912\n"
     ]
    }
   ],
   "source": [
    "# Fit SVC with default hyperparamters\n",
    "svc = svm.SVC(kernel=\"linear\", max_iter = 20)\n",
    "svc.fit(X_train, y_train)\n",
    "Y_preds_train = svc.predict(X_train)\n",
    "Y_preds_test  = svc.predict(X_test)\n",
    "\n",
    "\n",
    "# Define hyperparamters\n",
    "paramGrid = {\n",
    "    \"C\":      [ 10**-4, 10**-3, 10**-2, 10**-1] # Regularization term\n",
    "}\n",
    "\n",
    "#gridsearch\n",
    "gridSvc = GridSearchCV(svc, paramGrid, cv=5) \n",
    "gridSvc.fit(X_train, y_train) \n",
    "\n",
    "print(\"Best parameter of SVC:\"+ str(gridSvc.best_params_))\n",
    "\n",
    "#get best estimator\n",
    "svcBest = gridSvc.best_estimator_ \n",
    "\n",
    "#get scores\n",
    "scoreTrain = svcBest.score(X_train, y_train) \n",
    "scoreTest  = svcBest.score(X_test, y_test) \n",
    "\n",
    "print(\"Tain accuracy: \" + str(scoreTrain))\n",
    "print(\"Test accuarcy: \" +str(scoreTest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31d9e2f",
   "metadata": {},
   "source": [
    "### multi-layer perceptron classi\f",
    "er"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c9af97",
   "metadata": {},
   "outputs": [],
   "source": [
    "mlp = MLPClassifier(max_iter = 20)\n",
    "\n",
    "paramGrid = {\n",
    "        \"hidden_layer_sizes\":      [ (10,2),(10,1),(10,3),(50,1),(50,2)], # Regularization term\n",
    "    }\n",
    "    \n",
    "gridMlp = GridSearchCV(mlp, paramGrid, cv=5, n_jobs=-1) \n",
    "gridMlp.fit(X_train, y_train) \n",
    "\n",
    "print(f\"Best parameters of MLP:\", gridMlp.best_params_)\n",
    "\n",
    "mlpBest = gridMlp.best_estimator_ # Extract best model\n",
    "\n",
    "scoreTrain = mlpBest.score(X_train, y_train) \n",
    "scoreTest  = mlpBest.score(X_test, y_test) \n",
    "\n",
    "\n",
    "print(\"Tain accuracy: \" + str(scoreTrain))\n",
    "print(\"Test accuarcy: \" +str(scoreTest))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b25649ce",
   "metadata": {},
   "source": [
    "For SVM Model with the Regularization term C = {0.0001, 0.001, 0.01, 0.1} where tried and for die multi-layer-perceptron networks with the hidden layer shapes [ (10,2),(10,1),(10,3),(50,1),(50,2)] where trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "885eb619",
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the occurence of each possible Value of y \n",
    "df = pd.DataFrame(y_test, columns = ['y'])\n",
    "occur = df.groupby(['y']).size()\n",
    "# display occurrences of a particular column\n",
    "print(\"Appearence of Labels in Test Dataset: \")\n",
    "display(occur)\n",
    "best = 100/y_test.shape[0]*occur[4] \n",
    "print(\"Test performance that can be achieved using a constant prediction (in this case 4) is: \"+str(best))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b077a0",
   "metadata": {},
   "source": [
    "### Subtask 3: Classifer Evaluation I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d175eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion Matrix:\n",
    "\n",
    "def confusionMtx(yPredict, yTrue):\n",
    "    #create empty confusion Mtx\n",
    "    res = np.zeros([5,5] )\n",
    "    # Labes of 1-5 come in quit handy to define confusion Matrix by Indices. Add +1 for every Datapoint\n",
    "    for i in range(yPredict.shape[0]):\n",
    "        res[int(yTrue[i])-1, int(yPredict[i])-1] += 1\n",
    "    return res\n",
    "\n",
    "def reconstrucAccOfConfusionMtx(cMtx, dataLen):\n",
    "    rightClassified = 0\n",
    "    #summ over the diagonal elements\n",
    "    for i in range(cMtx.shape[0]):\n",
    "        rightClassified += cMtx[i,i]\n",
    "        #return acc\n",
    "    return 100/dataLen * rightClassified/100\n",
    "\n",
    "print(\"SVC confusion Matrix:\")\n",
    "print(confusionMtx(svcBest.predict(X_test), y_test))\n",
    "print(\"MLP confusion Matrix:\")\n",
    "print(confusionMtx(mlpBest.predict(X_test), y_test))\n",
    "\n",
    "print(\"reconstrucet accuaciy of SVC confusion Matrix:\")\n",
    "print(reconstrucAccOfConfusionMtx(confusionMtx(svcBest.predict(X_test), y_test), y_test.shape[0]))\n",
    "print(\"reconstrucet accuaciy of MLP confusion Matrix:\")\n",
    "print(reconstrucAccOfConfusionMtx(confusionMtx(mlpBest.predict(X_test), y_test), y_test.shape[0] ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a303baf",
   "metadata": {},
   "source": [
    "In the obtain confusion Matrix the structure becomes clear that the SVC classifier predicts ratings to be to hight and the MLP Matrix predicts Values to be to low. This probably relates to the low amount of learning iterations. However, doing more learning iterations would extend the runtime to high. If you have any hint for me on how I could decrease my training Runtime I would be very thankful."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
